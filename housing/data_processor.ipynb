{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic i/o\n",
    "import os\n",
    "\n",
    "# data structures\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# cleaning data\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# regressors\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# getting rid of annoying warnings.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "# data is first downloweded into DATA_PATH from \n",
    "# http://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "DATA_PATH = 'data'\n",
    "FILE_NAME = 'train.csv'\n",
    "\n",
    "def load_data(data_path=DATA_PATH, file_name=FILE_NAME, ratio=0.9):\n",
    "    # load everything into data\n",
    "    file_path = os.path.join(data_path, file_name)\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    slice_ind = int(data.shape[0]*ratio)\n",
    "    \n",
    "    return data.iloc[0:slice_ind], data.iloc[slice_ind:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data, _ = load_data(ratio=1)\n",
    "full_X = full_data.iloc[:,0:-1]\n",
    "full_y = full_data['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = load_data(ratio=0.9)\n",
    "train_X = train.iloc[:,0:-1]\n",
    "train_y = train['SalePrice']\n",
    "test_X = test.iloc[:,0:-1]\n",
    "test_y = test['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.columns == test_X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. The HousingDataProcessor class\n",
    "Below we implement the HousingData class. The purpose of this class is to write\n",
    "1. A data structure that contains the input data\n",
    "2. fit_transform raw data into processed data ready for ML application\n",
    "\n",
    "The fit functions consits of the following purpose:\n",
    "1. it assumes that 'SalePrice' is not a column of the data. The order of the columns of the data is fixed\n",
    "2. seperate the data into numerical and categorical data, records the names of these columns as self.num_ind and self.cat_ind\n",
    "3. fit by median of all ZERO_HEAVY_COLS columns\n",
    "4. fit numerical columns in (0,1) by MinMaxScaler, preserving the column order.\n",
    "5. fit by LabelEncoder and OneHotEncoder\n",
    "\n",
    "The transform_function does the transform part of the above. Returns a sparse matrix with everything in it\n",
    "1. it assumes that 'SalePrice' is not a column of the data. The order of the columns of the data is fixed\n",
    "2. seperate the data into numerical and categorical data via self.num_ind and self.cat_ind\n",
    "3. fill missing categorical data by the string 'UofTMath'\n",
    "4. fill all the missing numerical data by the median\n",
    "5. for each numerical column which has heavy data present at value 0, create a new column with entry 1 if heavy data is present at 0 and with entry 0 otherwise. Then set the entry in the original numerical column from 0 to the median. Finally, append to the end the new binary column to the category columns and name 'name_of_original_numeric_colum_zero'\n",
    "6. LabelEncoder and OneHotEncoder transform on categorical columns\n",
    "7. add a 'BIAS' column with only 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HousingDataProcessor(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # cleaning data\n",
    "        self.imputer_num = None\n",
    "        self.imputer_cat = None\n",
    "        self.imputer_zero = None\n",
    "        \n",
    "        self.scaler = None\n",
    "        self.regressor = None\n",
    "        self.Onehot_coders = {}\n",
    "        self.encoders = {}\n",
    "        self.ZERO_HEAVY_COLS = ['GarageArea', 'TotalBsmtSF', 'MasVnrArea', 'BsmtFinSF1', \n",
    "                       'WoodDeckSF', '2ndFlrSF', 'OpenPorchSF', 'BsmtUnfSF', \n",
    "                       'EnclosedPorch', 'ScreenPorch', 'PoolArea', '3SsnPorch', \n",
    "                       'LowQualFinSF', 'MiscVal', 'BsmtFinSF2', 'YearRemodAdd']\n",
    "        self.ZERO_HEAVY_COLS_zero = []\n",
    "    \n",
    "        for col in self.ZERO_HEAVY_COLS:\n",
    "            self.ZERO_HEAVY_COLS_zero.append(col + '_zero')\n",
    "    \n",
    "    # seperate_num_and_cat: self, pd.DataFrame --> list of strings, list of strings\n",
    "    # seperate numerical and non numerical columns in data\n",
    "    # returns nums, and cats. They are the indice of\n",
    "    # the numerical and categorical columns\n",
    "    def seperate_num_and_cat(self, data):\n",
    "        # numerical data type\n",
    "        cont_type = [int, np.int64, float, np.float64]\n",
    "\n",
    "        nums = []\n",
    "        cats = []\n",
    "\n",
    "        for col in data.columns:\n",
    "            if data[col].dtype in cont_type:\n",
    "                nums.append(col)\n",
    "                #data[col].to_numeric()\n",
    "            else:\n",
    "                cats.append(col)\n",
    "\n",
    "        return nums, cats #.astype(str)\n",
    "\n",
    "    # encode_fit: self, pd.DataFrame --> pd.DataFrame, list of LabelEncoder\n",
    "    # assumes input is a pd.DataFrame with dtype == str\n",
    "    # encodes each column in cats by LabelEncoder()\n",
    "    def encode_fit(self, cats):\n",
    "        encoders = {}\n",
    "\n",
    "        for cat in cats.columns:\n",
    "            encoder = LabelEncoder()\n",
    "            encoder.fit(cats[cat]) # won't work if cats has missing value\n",
    "            encoders[cat] = encoder\n",
    "        \n",
    "        return encoders\n",
    "        #return pd.DataFrame(encoded_cats) #, encoders\n",
    "    \n",
    "    # encode_transform: self, pd.DataFrame, list of LabelEncoder --> pd.DataFrame\n",
    "    def encode_transform(self, cats, encoders):\n",
    "        encoded_cats = {}\n",
    "\n",
    "        for cat in cats.columns:\n",
    "            encoded_cats[cat] = encoders[cat].transform(cats[cat]) # won't work if cats has missing value\n",
    "\n",
    "        return pd.DataFrame(encoded_cats, columns=cats.columns) #, encoders\n",
    "     \n",
    "\n",
    "    # encode_by_1hot_fit: list of string, pd.DataFrame --> dict of pd.sparseMatrix\n",
    "    # encodes by OneHotEncoder for each column in cat_ind in data\n",
    "    def encode_by_1hot_fit(self, cats):\n",
    "        Onehot_coders = {}\n",
    "\n",
    "        for col in cats.columns:\n",
    "            onehot_coder = OneHotEncoder()\n",
    "            onehot_coder.fit(cats[col].values.reshape(-1,1))\n",
    "            Onehot_coders[col] = onehot_coder\n",
    "\n",
    "        return Onehot_coders \n",
    "    \n",
    "    # encode_by_1hot: list of string, pd.DataFrame, list of OneHotEncoder --> dict of pd.sparseMatrix\n",
    "    def encode_by_1hot_transform(self, cats, Onehot_coders):\n",
    "        cat_1hot = []\n",
    "\n",
    "        for col in cats.columns:\n",
    "            cat_1hot.append(Onehot_coders[col].transform(cats[col].values.reshape(-1,1)))\n",
    "\n",
    "        return cat_1hot \n",
    "    \n",
    "    # add_0_indicators: pd.DataFrame --> void\n",
    "    # for col in ZERO_HEAVY_COLS, change all the zero to the median in data\n",
    "    # then make a new column in data with 1 == this entry was 0 in col and\n",
    "    # 0 == otherwise\n",
    "    # this is used to shift the zero heavy datas to the median,\n",
    "    # in an attempt to correct skewedness\n",
    "    def zero_indicators_fit(self, data):\n",
    "        medians = {}\n",
    "        \n",
    "        for col in self.ZERO_HEAVY_COLS:\n",
    "            medians[col] = data[col][data[col] > 0].median()\n",
    "        \n",
    "        return medians\n",
    "            \n",
    "    def zero_indicators_transform(self, data, medians):   \n",
    "        for col in self.ZERO_HEAVY_COLS:\n",
    "            new_col = col + '_zero'\n",
    "            data[new_col] = 1 * (data[col] == 0)\n",
    "\n",
    "            data.loc[data[col]==0, col] = medians[col]\n",
    "\n",
    "    \n",
    "    def add_0_indicators(self, data):   \n",
    "        for col in self.ZERO_HEAVY_COLS:\n",
    "            new_col = col + '_zero'\n",
    "            data[new_col] = 1 * (data[col] == 0)\n",
    "\n",
    "    \n",
    "        \n",
    "    def fit_(self, data):        \n",
    "        # find numeric and categorical columns of data\n",
    "        nums_ind, cat_ind = self.seperate_num_and_cat(data)\n",
    "\n",
    "        # in numerical data: fill nan by median\n",
    "        self.imputer_num = SimpleImputer(strategy='median')       \n",
    "        data[nums_ind] = self.imputer_num.fit_transform(data[nums_ind])\n",
    "\n",
    "        # in categorical data: fill nan by 'nan'\n",
    "        self.imputer_cat = SimpleImputer(strategy='constant', fill_value='UofTMath') \n",
    "        data[cat_ind] = self.imputer_cat.fit_transform(data[cat_ind])\n",
    "        \n",
    "        # change all the zeros to median and make a new column to indicate the appearance of 0\n",
    "        self.add_0_indicators(data)\n",
    "        cat_ind += self.ZERO_HEAVY_COLS_zero\n",
    "        self.imputer_zero = SimpleImputer(strategy='median', missing_values=0)\n",
    "        data[self.ZERO_HEAVY_COLS] = self.imputer_zero.fit_transform(data[self.ZERO_HEAVY_COLS])\n",
    "    \n",
    "        # scale the numerical columns to 0 mean and 1 standard deviation\n",
    "        #self.scaler = MinMaxScaler()\n",
    "        #data[nums_ind] = \\\n",
    "        #    pd.DataFrame(self.scaler.fit_transform(data[nums_ind]), columns=nums_ind)\n",
    "        \n",
    "        \n",
    "        # encode categories\n",
    "        self.encoders = self.encode_fit(data[cat_ind])\n",
    "        data_cat = self.encode_transform(data[cat_ind], self.encoders)\n",
    "          \n",
    "        \n",
    "        # 1hot encode\n",
    "        self.Onehot_coders = self.encode_by_1hot_fit(data_cat)\n",
    "        \n",
    "    def fit(self, X):\n",
    "        data = X.copy()\n",
    "        del data['Id']\n",
    "        \n",
    "        self.fit_(data)\n",
    " \n",
    "    def transform_(self, X, debug=False):\n",
    "        if debug:\n",
    "            print('X start')\n",
    "            print(X.isnull().values.any())\n",
    "            #print(X.head())\n",
    "        \n",
    "        # find numeric and categorical columns of data\n",
    "        nums_ind, cat_ind = self.seperate_num_and_cat(X)\n",
    "        \n",
    "        # in numerical data: fill nan by median     \n",
    "        X[nums_ind] = self.imputer_num.transform(X[nums_ind])\n",
    "\n",
    "        # in categorical data: fill nan by 'nan'\n",
    "        X[cat_ind] = self.imputer_cat.transform(X[cat_ind])\n",
    "        \n",
    "        if debug:\n",
    "            print('X after imputation')\n",
    "            print(X.isnull().values.any())\n",
    "            #print(X.head())\n",
    "        \n",
    "        # change all the zeros to median and make a new column to indicate the appearance of 0\n",
    "        self.add_0_indicators(X)\n",
    "        cat_ind += self.ZERO_HEAVY_COLS_zero\n",
    "        X[self.ZERO_HEAVY_COLS] = self.imputer_zero.transform(X[self.ZERO_HEAVY_COLS])\n",
    "        \n",
    "        if debug:\n",
    "            print('X after adding zeros')\n",
    "            print(X.isnull().values.any())\n",
    "            #print(X.head())\n",
    "        \n",
    "        # I don't know why but this gives an array full of nan sometimes!!!!!!!\n",
    "        # scale the numerical columns between 0 and 1\n",
    "        #X[nums_ind] = \\\n",
    "            #pd.DataFrame(self.scaler.transform(X[nums_ind]), columns=nums_ind)\n",
    "        \n",
    "                \n",
    "        if debug:\n",
    "            print('X after rescaling')\n",
    "            print(X.isnull().values.any())\n",
    "            #print(X.head())\n",
    "\n",
    "        \n",
    "        # encode categories\n",
    "        data_cat = self.encode_transform(X[cat_ind], self.encoders)\n",
    "        \n",
    "        # this is a list of sparse 1hot mat\n",
    "        data_onehot = self.encode_by_1hot_transform(data_cat, self.Onehot_coders)\n",
    "        \n",
    "        # add a bias column\n",
    "        X['BIAS'] = 1\n",
    "        nums_ind.append('BIAS')\n",
    "        \n",
    "        if debug:\n",
    "            print('X before sparsify')\n",
    "            print(X.isnull().values.any())\n",
    "        \n",
    "        \n",
    "        \n",
    "        # put the numerical dense matrix in sparse format so that we can concatenate it with the 1hot encodings\n",
    "        data_num = sparse.csr_matrix(X[nums_ind].values)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ans = sparse.hstack([data_num] + data_onehot)\n",
    "        \n",
    "        if debug:\n",
    "            print('final step')\n",
    "            print(np.isnan(ans.toarray()).any())\n",
    "        \n",
    "        return ans\n",
    "    \n",
    "    def transform(self, X):\n",
    "        data = X.copy()\n",
    "        del data['Id']\n",
    "        \n",
    "        return self.transform_(data)\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "    \n",
    "    def preprocess_label(self, y):\n",
    "        return np.log(y)\n",
    "        \n",
    "    \n",
    "    def postprocess_label(self, y):\n",
    "        return np.exp(y)\n",
    "\n",
    "        \n",
    "    def predict(self, data):\n",
    "        X, _ = self.preprocess_data(data)\n",
    "        \n",
    "        return self.regressor.predict(X)\n",
    "\n",
    "        \n",
    "    def input_data(self, X):\n",
    "        self.training_data = self.preprocess_training_data(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. HousingRegressor class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HousingRegressor(object):\n",
    "    def __init__(self, full_data, n_estimators=10):\n",
    "        self.dataProcessor = HousingDataProcessor()\n",
    "        X = full_data.copy()\n",
    "        del X['SalePrice']\n",
    "        \n",
    "        self.dataProcessor.fit(X)\n",
    "        \n",
    "        self.regressor = None\n",
    "        self.n_estimators = n_estimators\n",
    "        \n",
    "        \n",
    "    def fit_(self, X, y):\n",
    "        self.regressor = BaggingRegressor(base_estimator=GradientBoostingRegressor(min_samples_split=2, # this is default \n",
    "                                                                               max_depth=3 # this is default\n",
    "                                                                              ), \n",
    "                                      n_estimators=self.n_estimators, # default is 10\n",
    "                                      max_features=1.0, # default is all or 1.0\n",
    "                                      n_jobs=-1\n",
    "                                     )\n",
    "        \n",
    "        self.regressor.fit(X, y)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X = self.dataProcessor.transform(X)\n",
    "        self.fit_(X, np.log(y.values))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = self.dataProcessor.transform(X)\n",
    "        return np.exp(self.regressor.predict(X))\n",
    "    \n",
    "    def predictionError(self, X, y):\n",
    "        ypred = self.predict(X)\n",
    "        return np.sqrt(np.sum((np.log(ypred) - np.log(y.values)) ** 2)/y.shape[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_data, _ = load_data(data_path=DATA_PATH, file_name='test.csv', ratio=1)\n",
    "submit_data['SalePrice'] = 0\n",
    "data_str = pd.concat([full_data, submit_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = HousingRegressor(data_str, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.fit(full_X, full_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('error:',reg.predictionError(test_X, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del submit_data['SalePrice']\n",
    "length = submit_data.shape[0]\n",
    "\n",
    "pred = reg.predict(submit_data)\n",
    "submit = pd.DataFrame({'Id': np.arange(1461, 1461+length), 'SalePrice' : pred})\n",
    "submit.to_csv('02092019.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
